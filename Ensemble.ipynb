{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKYWuRj_Fgrh"
      },
      "source": [
        "# <span style='color:orange'>  앙상블(Ensemble) 기법 :<span style='color:red'> 과적합(Overfitting) 방지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JyrLltcFgrz"
      },
      "source": [
        "# <span style='color:orange'> 앙상블기법:\n",
        "## <span style='color:purple'>   -  Boosting: Bias 조정 (AdaBoost, GradientBoost, XGBoost)\n",
        "##  <span style='color:green'>   - Bagging : Variance 조정 (Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yn1C1oEFgr2"
      },
      "source": [
        "# <span style= 'color: orange'>  부스팅의 가중치에 따라 분류: AdaBoost, GradientBoost, XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNgbvDCFgr4"
      },
      "source": [
        "# <span style= 'color: orange'> xgboost 설치:\n",
        "##   <span style= 'color: purple'>  pip install xgboost\n",
        "\n",
        "##   <span style= 'color: purple'>  conda install -c anaconda py-xgboost=0.72"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u-Ns6xjFgr5",
        "outputId": "65062ca8-e55e-40d3-dfb7-fca33033d33d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
            "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
            "    shrinking=True, tol=0.001, verbose=False) full 변수 정확도:\n",
            "0.8621103117505995\n",
            "\n",
            "\n",
            "GaussianNB(priors=None, var_smoothing=1e-09) full 변수 정확도:\n",
            "0.8621103117505995\n",
            "\n",
            "\n",
            "NearestCentroid(metric='euclidean', shrink_threshold=None) full 변수 정확도:\n",
            "0.6223021582733813\n",
            "\n",
            "\n",
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform') full 변수 정확도:\n",
            "0.8800959232613909\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
            "                   warm_start=False) full 변수 정확도:\n",
            "0.8717026378896883\n",
            "\n",
            "\n",
            "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best') full 변수 정확도:\n",
            "0.9124700239808153\n",
            "\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False) full 변수 정확도:\n",
            "0.9592326139088729\n",
            "\n",
            "\n",
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
            "0.8717026378896883\n",
            "\n",
            "\n",
            "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                           n_iter_no_change=None, presort='auto',\n",
            "                           random_state=None, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False) full 변수 정확도:\n",
            "0.9556354916067147\n",
            "\n",
            "\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
            "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
            "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
            "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
            "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
            "0.9532374100719424\n",
            "\n",
            "\n",
            "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
            "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                     min_samples_leaf=1, min_samples_split=2,\n",
            "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
            "                     oob_score=False, random_state=None, verbose=0,\n",
            "                     warm_start=False) full 변수 정확도:\n",
            "0.920863309352518\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df=pd.read_csv('Churn.csv')\n",
        "df=np.array(df)\n",
        "X, y=df[:, 1:], df[:, 0]\n",
        "#iris = load_iris()\n",
        "#X, y = iris.data, iris.target\n",
        "\n",
        "clf1 = svm.SVC()\n",
        "clf2 = GaussianNB()\n",
        "clf3 = NearestCentroid()\n",
        "clf4 = KNeighborsClassifier()\n",
        "clf5 = LogisticRegression()\n",
        "clf6 = DecisionTreeClassifier()\n",
        "clf7 = RandomForestClassifier()\n",
        "clf8 = AdaBoostClassifier()\n",
        "clf9 = GradientBoostingClassifier()\n",
        "clf10 = XGBClassifier()\n",
        "clf11= ExtraTreesClassifier()\n",
        "for i in range(1):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n",
        "    for clf in [clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9, clf10, clf11]:\n",
        "        clf.fit(X_train, y_train)\n",
        "        print(clf,\"full 변수 정확도:\")\n",
        "        print(clf.score(X_test, y_test))\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFivvIJeFgr8",
        "outputId": "242dd855-2873-46b8-b0e7-909253ae5bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best') full 변수 정확도:\n",
            "0.9184652278177458\n",
            "\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False) full 변수 정확도:\n",
            "0.9508393285371702\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
            "0.8717026378896883\n",
            "\n",
            "\n",
            "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                           n_iter_no_change=None, presort='auto',\n",
            "                           random_state=None, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False) full 변수 정확도:\n",
            "0.9556354916067147\n",
            "\n",
            "\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
            "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
            "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
            "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
            "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
            "0.9532374100719424\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "df=pd.read_csv('Churn.csv')\n",
        "df=np.array(df)\n",
        "X, y=df[:, 1:], df[:, 0]\n",
        "#iris = load_iris()\n",
        "#X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=1/4,random_state=0)\n",
        "\n",
        "#clf1 = svm.SVC()\n",
        "#clf2 = GaussianNB()\n",
        "clf3 = AdaBoostClassifier()\n",
        "clf4 = GradientBoostingClassifier()\n",
        "clf5 = XGBClassifier()\n",
        "clf1 = DecisionTreeClassifier()\n",
        "clf2 = RandomForestClassifier()\n",
        "for i in range(1):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n",
        "    for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
        "        clf.fit(X_train, y_train)\n",
        "        print(clf,\"full 변수 정확도:\")\n",
        "        print(clf.score(X_test, y_test))\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwsOztW7Fgr9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYZ86tEsFgr9"
      },
      "source": [
        "# <span style= 'color: orange'> Imbalanced Classification : Oversampling (Bagging, SMOTE, cGAN)\n",
        "  <a href=\"https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&cad=rja&uact=8&ved=2ahUKEwiGvsPHia3hAhWyKqYKHbX-AkgQFjADegQIAhAC&url=http%3A%2F%2Fwww.databaser.net%2Fmoniwiki%2Fpds%2FDataAnalysis%2Fmining_process_01.pdf&usg=AOvVaw2hWdiY31JVLVZDtgaaEOTH\"> 불균형 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7erdpShhFgr9",
        "outputId": "2f05ea6a-f893-4cf9-e59a-9185e69a9a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3333, 17) (483, 17) (3816, 17)\n",
            "0.7809224318658281\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df=pd.read_csv('Churn.csv')\n",
        "#display(df)\n",
        "#df.pivot_table(rows=['Churn'])\n",
        "dz=df[df.Churn> 0]\n",
        "#display(dz)\n",
        "dx=df.append(dz)\n",
        "#display(dx)\n",
        "#dz.to_csv('Churn1.csv')\n",
        "#dx.to_csv('Churn2.csv')\n",
        "#pd.merge(df, dz)\n",
        "#display(df)\n",
        "print(df.shape, dz.shape, dx.shape)\n",
        "\n",
        "#print(df.groupby('Churn').count())\n",
        "dx=np.array(dx)\n",
        "#print(df.groupby[:,0].sum())\n",
        "X=dx[:, 1:]\n",
        "y=dx[:, 0]\n",
        "#display(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y) # 씨드 랜덤\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bto7oIKIFgr-",
        "outputId": "caff8ff3-ae21-4be2-a5f7-a17898053580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3333, 17) (483, 17) (3816, 17)\n",
            "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best') full 변수 정확도:\n",
            "0.9266247379454927\n",
            "\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False) full 변수 정확도:\n",
            "0.9517819706498952\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
            "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
            "0.8511530398322851\n",
            "\n",
            "\n",
            "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                           n_iter_no_change=None, presort='auto',\n",
            "                           random_state=None, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False) full 변수 정확도:\n",
            "0.9339622641509434\n",
            "\n",
            "\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
            "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
            "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
            "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
            "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
            "0.9245283018867925\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "df=pd.read_csv('Churn.csv')\n",
        "dz=df[df.Churn> 0]\n",
        "dx=df.append(dz)\n",
        "print(df.shape, dz.shape, dx.shape)\n",
        "#print(df.groupby('Churn').count())\n",
        "dx=np.array(dx)\n",
        "#print(df.groupby[:,0].sum())\n",
        "X=dx[:, 1:]\n",
        "y=dx[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=1/4, random_state=0)\n",
        "#clf1 = svm.SVC()\n",
        "#clf2 = GaussianNB()\n",
        "clf3 = AdaBoostClassifier()\n",
        "clf4 = GradientBoostingClassifier()\n",
        "clf5 = XGBClassifier()\n",
        "clf1 = DecisionTreeClassifier()\n",
        "clf2 = RandomForestClassifier()\n",
        "for i in range(1):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4,random_state=0)\n",
        "    for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
        "        clf.fit(X_train, y_train)\n",
        "        print(clf,\"full 변수 정확도:\")\n",
        "        print(clf.score(X_test, y_test))\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xZSGWJSFgr_"
      },
      "source": [
        "# SMOTE(Synthetic Minority Oversampling TEchnique)\n",
        "\n",
        "# PCA Decomposition\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLBDLI0iFgr_"
      },
      "source": [
        "# <span style='color:orange'> Stacking-Bagging"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}