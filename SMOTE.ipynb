{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3333, 17) (483, 17) (3816, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,  25.  , 265.1 , ...,  11.01,   3.  ,   2.7 ],\n",
       "       [  0.  ,  26.  , 161.6 , ...,  11.45,   3.  ,   3.7 ],\n",
       "       [  0.  ,   0.  , 243.4 , ...,   7.32,   5.  ,   3.29],\n",
       "       ...,\n",
       "       [  1.  ,   0.  , 140.  , ...,   5.4 ,   4.  ,   2.62],\n",
       "       [  1.  ,   0.  , 321.1 , ...,   8.12,   2.  ,   3.11],\n",
       "       [  1.  ,   0.  , 118.4 , ...,  10.22,   3.  ,   3.67]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best') full 변수 정확도:\n",
      "0.9549266247379455\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) full 변수 정확도:\n",
      "0.9643605870020965\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
      "0.8375262054507338\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) full 변수 정확도:\n",
      "0.9371069182389937\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
      "0.9308176100628931\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(random_state=0)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('Churn.csv')\n",
    "\n",
    "dz=df[df.Churn> 0]\n",
    "dx=df.append(dz)\n",
    "print(df.shape, dz.shape, dx.shape)\n",
    "\n",
    "#print(df.groupby('Churn').count())\n",
    "dx=np.array(dx)\n",
    "display(dx)\n",
    "np.random.shuffle(dx)\n",
    "\n",
    "#print(df.groupby[:,0].sum())\n",
    "X=dx[:, 1:] \n",
    "y=dx[:, 0]\n",
    "\n",
    "#_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=1/4)\n",
    "\n",
    "#X_train_s, y_train_s=smote.fit_sample(X_train, y_train)\n",
    "#clf1 = svm.SVC()\n",
    "#clf2 = GaussianNB()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4,random_state=0)\n",
    "#    X_train_s, y_train_s=smote.fit_sample(X_train, y_train)\n",
    "    for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(clf,\"full 변수 정확도:\")\n",
    "        print(clf.score(X_test, y_test))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3333, 17) (483, 17) (3816, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,  25.  , 265.1 , ...,  11.01,   3.  ,   2.7 ],\n",
       "       [  0.  ,  26.  , 161.6 , ...,  11.45,   3.  ,   3.7 ],\n",
       "       [  0.  ,   0.  , 243.4 , ...,   7.32,   5.  ,   3.29],\n",
       "       ...,\n",
       "       [  1.  ,   0.  , 140.  , ...,   5.4 ,   4.  ,   2.62],\n",
       "       [  1.  ,   0.  , 321.1 , ...,   8.12,   2.  ,   3.11],\n",
       "       [  1.  ,   0.  , 118.4 , ...,  10.22,   3.  ,   3.67]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best') full 변수 정확도:\n",
      "0.9290382819794585\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) full 변수 정확도:\n",
      "0.9551820728291317\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
      "0.8664799253034547\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) full 변수 정확도:\n",
      "0.9365079365079365\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
      "0.930905695611578\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(random_state=0)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('Churn.csv')\n",
    "\n",
    "dz=df[df.Churn> 0]\n",
    "dx=df.append(dz)\n",
    "print(df.shape, dz.shape, dx.shape)\n",
    "\n",
    "#print(df.groupby('Churn').count())\n",
    "dx=np.array(dx)\n",
    "display(dx)\n",
    "np.random.shuffle(dx)\n",
    "\n",
    "#print(df.groupby[:,0].sum())\n",
    "X=dx[:, 1:] \n",
    "y=dx[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=1/4)\n",
    "\n",
    "X_train_s, y_train_s=smote.fit_sample(X_train, y_train)\n",
    "#clf1 = svm.SVC()\n",
    "#clf2 = GaussianNB()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_s, y_train_s, test_size=1/4,random_state=0)\n",
    "    #    X_train_s, y_train_s=smote.fit_sample(X_train, y_train)\n",
    "    for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(clf,\"full 변수 정확도:\")\n",
    "        print(clf.score(X_test, y_test))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3333, 17) (483, 17) (3816, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,  25.  , 265.1 , ...,  11.01,   3.  ,   2.7 ],\n",
       "       [  0.  ,  26.  , 161.6 , ...,  11.45,   3.  ,   3.7 ],\n",
       "       [  0.  ,   0.  , 243.4 , ...,   7.32,   5.  ,   3.29],\n",
       "       ...,\n",
       "       [  1.  ,   0.  , 140.  , ...,   5.4 ,   4.  ,   2.62],\n",
       "       [  1.  ,   0.  , 321.1 , ...,   8.12,   2.  ,   3.11],\n",
       "       [  1.  ,   0.  , 118.4 , ...,  10.22,   3.  ,   3.67]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best') full 변수 정확도:\n",
      "1.0\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) full 변수 정확도:\n",
      "0.9990627928772259\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None) full 변수 정확도:\n",
      "0.8716026241799437\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) full 변수 정확도:\n",
      "0.9409559512652296\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              seed=None, silent=True, subsample=1) full 변수 정확도:\n",
      "0.9381443298969072\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(random_state=0)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv('Churn.csv')\n",
    "\n",
    "dz=df[df.Churn> 0]\n",
    "dx=df.append(dz)\n",
    "print(df.shape, dz.shape, dx.shape)\n",
    "\n",
    "#print(df.groupby('Churn').count())\n",
    "dx=np.array(dx)\n",
    "display(dx)\n",
    "np.random.shuffle(dx)\n",
    "\n",
    "#print(df.groupby[:,0].sum())\n",
    "X=dx[:, 1:] \n",
    "y=dx[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=1/4)\n",
    "\n",
    "X_train_s, y_train_s=smote.fit_sample(X_train, y_train)\n",
    "#clf1 = svm.SVC()\n",
    "#clf2 = GaussianNB()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_s, y_train_s, test_size=1/4,random_state=0)\n",
    "    for clf in [clf1, clf2, clf3, clf4, clf5]:\n",
    "        clf.fit(X_train_s, y_train_s)\n",
    "        print(clf,\"full 변수 정확도:\")\n",
    "        print(clf.score(X_test, y_test))\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
